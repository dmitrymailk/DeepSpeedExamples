{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/xglm-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2, 113677,   3038]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
    "model_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "from flash_attn.bert_padding import unpad_input, pad_input\n",
    "\n",
    "# для того чтобы это заработало нужно открыть исходники и закоментировать все упоминания \n",
    "# TORCH_CHECK из сурсов, а затем скомпилировать это\n",
    "# искать в файле csrc/flash_attn/fmha_api.cpp\n",
    "def flash_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.Tensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    **other_keys\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    \"\"\"Input shape: Batch x Time x Channel\n",
    "\n",
    "    attention_mask: [bsz, q_len]\n",
    "    \"\"\"\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = (\n",
    "        self.q_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    key_states = (\n",
    "        self.k_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    value_states = (\n",
    "        self.v_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    assert past_key_value is None, \"past_key_value is not supported\"\n",
    "    assert not output_attentions, \"output_attentions is not supported\"\n",
    "    assert not use_cache, \"use_cache is not supported\"\n",
    "\n",
    "    # Flash attention codes from\n",
    "    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n",
    "\n",
    "    # transform the data into the format required by flash attention\n",
    "    qkv = torch.stack(\n",
    "        [query_states, key_states, value_states], dim=2\n",
    "    )  # [bsz, nh, 3, q_len, hd]\n",
    "    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n",
    "    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n",
    "    # the attention_mask should be the same as the key_padding_mask\n",
    "    key_padding_mask = attention_mask\n",
    "\n",
    "    if key_padding_mask is None:\n",
    "        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n",
    "        max_s = q_len\n",
    "        cu_q_lens = torch.arange(\n",
    "            0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device\n",
    "        )\n",
    "        output = flash_attn_unpadded_qkvpacked_func(\n",
    "            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n",
    "        )\n",
    "        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n",
    "    else:\n",
    "        nheads = qkv.shape[-2]\n",
    "        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n",
    "        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n",
    "        x_unpad = rearrange(\n",
    "            x_unpad,\n",
    "            \"nnz (three h d) -> nnz three h d\",\n",
    "            three=3,\n",
    "            h=nheads,\n",
    "        )\n",
    "        output_unpad = flash_attn_unpadded_qkvpacked_func(\n",
    "            x_unpad,\n",
    "            cu_q_lens,\n",
    "            max_s,\n",
    "            0.0,\n",
    "            softmax_scale=None,\n",
    "            causal=True,\n",
    "        )\n",
    "        output = rearrange(\n",
    "            pad_input(\n",
    "                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"),\n",
    "                indices,\n",
    "                bsz,\n",
    "                q_len,\n",
    "            ),\n",
    "            \"b s (h d) -> b s h d\",\n",
    "            h=nheads,\n",
    "        )\n",
    "    return self.out_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, None\n",
    "\n",
    "\n",
    "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n",
    "# requires the attention mask to be the same as the key_padding_mask\n",
    "def _prepare_decoder_attention_mask(\n",
    "    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "):\n",
    "    # [bsz, seq_len]\n",
    "    return attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.models.xglm.modeling_xglm.XGLMAttention.forward = flash_forward\n",
    "transformers.models.xglm.modeling_xglm.XGLMModel._prepare_decoder_attention_mask = (\n",
    "    _prepare_decoder_attention_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 2048, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XGLMForCausalLM\n",
    "\n",
    "model = XGLMForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss={'logits': tensor([[[ -0.3433,  -0.4675, 109.5000,  ...,  -0.7705,  -1.8184,  -1.6680],\n",
       "         [  3.4414,   3.0742, 234.7500,  ...,   1.6592,  -0.4905,   1.4746],\n",
       "         [  3.8652,   3.6523, 241.8750,  ...,   2.3770,   0.2998,   2.6523]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), 'past_key_values': (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)}, logits=tensor([[[ -0.3433,  -0.4675, 109.5000,  ...,  -0.7705,  -1.8184,  -1.6680],\n",
       "         [  3.4414,   3.0742, 234.7500,  ...,   1.6592,  -0.4905,   1.4746],\n",
       "         [  3.8652,   3.6523, 241.8750,  ...,   2.3770,   0.2998,   2.6523]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), past_key_values=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**model_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
