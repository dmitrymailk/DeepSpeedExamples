{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/HuggingFaceH4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yaodongC/awesome-instruction-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- databricks/databricks-dolly-15k - https://huggingface.co/datasets/databricks/databricks-dolly-15k\n",
    "- self_instruct - https://huggingface.co/datasets/yizhongw/self_instruct\n",
    "- OpenAssistant/oasst1 - https://huggingface.co/datasets/OpenAssistant/oasst1\n",
    "- sahil2801/CodeAlpaca-20k - https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\n",
    "- HuggingFaceH4/helpful_instructions - https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions\n",
    "- HuggingFaceH4/stack-exchange-preferences - https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences\n",
    "- openai/summarize_from_feedback - https://huggingface.co/datasets/openai/summarize_from_feedback \n",
    "- stanfordnlp/SHP - https://huggingface.co/datasets/stanfordnlp/SHP\n",
    "- unnatural-instructions(text-davinci002) - https://github.com/orhonovich/unnatural-instructions/tree/main\n",
    "- eli5 - https://huggingface.co/datasets/eli5\n",
    "- chip2_instruct_alpha - https://github.com/Rallio67/language-model-agents/tree/main\n",
    "\n",
    "- нужно больше датасетов с саммаризацией\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, \n",
    "        model_name: str,\n",
    "        device = 'cuda'\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.init()\n",
    "    \n",
    "    def init(self):\n",
    "        print(\"Init model.\")\n",
    "        if self.model_name == \"facebook/nllb-200-3.3B\":\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.model_name, \n",
    "                use_auth_token=True,\n",
    "            )\n",
    "            self.model.eval()\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                use_auth_token=True,\n",
    "            )\n",
    "        print(\"Model is initialized.\")\n",
    "    \n",
    "    def translate(self, text: str):\n",
    "        with torch.no_grad():\n",
    "            if self.model_name == \"facebook/nllb-200-3.3B\":\n",
    "                return self.nllb_translate(text=text)\n",
    "    \n",
    "    def __call__(self, text: str):\n",
    "        return self.translate(text=text)\n",
    "    \n",
    "    def nllb_translate(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = self.to_device(inputs=inputs)\n",
    "        translated_tokens = self.model.generate(\n",
    "            **inputs, \n",
    "            forced_bos_token_id=self.tokenizer.lang_code_to_id[\"rus_Cyrl\"],\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    def to_device(self, inputs):\n",
    "        for key in inputs.keys():\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is initialized.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "device = torch.device(\"cuda:1\")\n",
    "translator = Translator(model_name=model_name, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1336: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Привет, мир.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"hello world\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### databricks/databricks-dolly-15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/kosenko/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-aae1918f8081f1c6/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 450.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(data[\"train\"].shuffle()):\n",
    "\tfields = [\"context\", \"instruction\", \"response\"]\n",
    "\tfor field in fields:\n",
    "\t\tprint(f\"Field name: {field}\")\n",
    "\t\tprint(\"Original: \", example[field])\n",
    "\t\ttext = example[field]\n",
    "\t\ttranslated = translator(text=text)\n",
    "\t\tprint(\"Translated: \", translated)\n",
    "\t\tprint()\n",
    "\tprint(\"==\" * 100)\n",
    "\n",
    "\tif i > 100:\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "torch.Size([8, 16, 512, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True,\n",
    "            enable_math=False,\n",
    "            enable_mem_efficient=True,\n",
    "        ):\n",
    "    print(\"hello\")\n",
    "    # bsz, tgt_len, _ = hidden_states.size()\n",
    "    query = torch.randn(8, 16, 512, 128) # (batch_size, num_heads, embed_dim)\n",
    "    key = torch.randn(8, 16, 512, 128) # (batch_size, num_heads, embed_dim)\n",
    "    value = torch.randn(8, 16, 512, 128) # (batch_size, num_heads, embed_dim)\n",
    "    y = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0,\n",
    "        is_causal=True,\n",
    "    )\n",
    "    print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
